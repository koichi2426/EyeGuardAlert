from flask import Flask, render_template, request
import cv2
import cvzone
from cvzone.FaceMeshModule import FaceMeshDetector
from cvzone.PlotModule import LivePlot



app = Flask(__name__)

def run_camera():
    # ここにカメラ関数のコードを貼り付けてください

    # カメラをキャプチャするためのVideoCaptureオブジェクトを作成
    # このコード行は、OpenCVを使用してカメラからビデオをキャプチャするために使われます。具体的には、カメラデバイスを開き、ビデオフィードを取得するための設定を行います。
    # ここでの主要なポイントは、`cv2.VideoCapture()` 関数の引数です。
    # - `0` は、カメラデバイスのインデックスを指定しています。通常、複数のカメラが接続されている場合、0はデフォルトのカメラ（通常はウェブカメラ）を指します。他のカメラを使用する場合、1、2、3などの値を指定できます。
    # - `cv2.CAP_DSHOW` は、カメラキャプチャ用のバックエンドとしてDirectShowを使用することを指定しています。DirectShowは、Windowsプラットフォームでカメラデバイスと対話するための一般的なAPIです。
    # この行を実行すると、カメラからのビデオフィードがキャプチャされ、それを後続の処理に使用できます。このビデオフィードは、顔メッシュを検出してまばたきを検出するために使用されています。
    cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)

    # FaceMesh検出器を初期化し、最大で1つの顔を検出するように設定
    detector = FaceMeshDetector(maxFaces=1)

    # グラフを表示するためのLivePlotオブジェクトを初期化
    # このコード行は、cvzoneライブラリを使用してライブプロットを設定しています。ライブプロットは、リアルタイムでデータをグラフとして表示するために使用されます。
    # 以下はこの行の主要な要素です：
    # - `LivePlot(640,360,[20,50], invert=True)` は、`LivePlot` クラスのコンストラクタを呼び出しています。
    # - `640` と `360` は、プロットウィンドウの幅と高さをピクセル単位で指定しています。これにより、プロットの表示領域のサイズが設定されます。
    # - `[20, 50]` は、Y軸のプロット範囲を指定しています。つまり、Y軸の値は20から50の範囲内で変動します。
    # - `invert=True` は、Y軸の値の反転を有効にする設定です。この設定が有効になると、Y軸の値が上から下に向かって増加するのではなく、下から上に向かって増加します。通常、時間経過に伴うデータをリアルタイムで表示する際に使用されます。
    # このライブプロットを使用することで、まばたきの比率をリアルタイムでグラフ化し、まばたきのトレンドを可視化できます。ライブプロットは、データの動きを視覚的に追跡するために非常に便利です。
    plotY = LivePlot(640,360,[20,50], invert=True)

    # まばたきを検出するための目のランドマークのインデックスのリスト
    #
    #　＜ランドマークとは＞
    # 「ランドマーク（landmark）」は、顔や物体などの特定のポイントや位置を指す用語です。特に、顔のランドマークは顔の異なる部分や特定の位置を示す点や特徴です。顔のランドマークは、顔の形状や特徴を数値化したり、顔認識、顔検出、表情分析、姿勢推定などのコンピュータビジョンのアプリケーションで使用されます。
    # 顔のランドマークは、通常、以下のようなものを含みます：
    # 1. 目のキャント（外側の角度から内側の角度までの線）
    # 2. 鼻の先
    # 3. 鼻の根本
    # 4. 口の両端
    # 5. 顎の先端
    # 6. 眉毛の位置
    # 7. 顔の輪郭（顔の輪郭を形成する複数のポイント）
    # ランドマークの位置情報は通常、数値の座標で表され、コンピュータアルゴリズムや機械学習モデルによって検出され、使用されます。例えば、表情認識では、顔のランドマークの動きや位置の変化を分析して、表情を推定するのに使用されます。また、まばたきの検出や顔の姿勢推定などのタスクにもランドマーク情報が利用されます。
    # FaceMeshというライブラリやモジュールは、顔のランドマークを検出するために使用され、顔の形状をリアルタイムでトラッキングするのに役立ちます。

    # このコードは、目のランドマークのインデックスと、まばたきの比率を格納するためのリストを初期化しています。
    # 1. `idList` リストは、目のランドマークのインデックスを格納します。これらのインデックスは、FaceMeshモデルが顔の中で特定のランドマークを識別するのに使用します。各番号は特定のランドマークを指します。
    # 2. `ratioList` リストは、まばたきの比率を格納するために使用されます。この比率は、目のランドマークの長さの垂直方向と水平方向の比を表します。この比率はまばたきの検出に使用され、まばたきの状態を追跡します。リストは後で直近の比率を計算するために使用されます。
    # これらのリストは、目のランドマークとまばたきのデータを管理し、まばたきの検出とカウントのために使用されます。ランドマークのインデックスは、目の部分を正確に識別し、その後の処理で使用するために重要です。そして、比率リストは、まばたきの状態をトラッキングするために計算され、まばたき回数のカウントに役立ちます。
    idList = [22,23,24,26,110,157,158,159,160,130,243]
    ratioList = []

    # まばたきカウンターとフレームカウンターの初期化
    # このコードでは、2つの変数 `blinkCounter` と `counter` が初期化されています。これらの変数は、まばたきのカウントと関連する情報を追跡するために使用されます。
    # 1. `blinkCounter`: まばたきの回数を格納するための変数です。初期値は0です。この変数は、まばたきが検出されるたびに増加し、ユーザーがまばたきした回数を記録します。
    # 2. `counter`: まばたきのカウントに関連する補助的なカウンター変数です。この変数は、まばたきの検出後、一時的にカウントがリセットされるために使用されます。まばたきを検出した後、`counter` は1から始まり、カウントが10に達したら再び0にリセットされます。このようにして、1回のまばたきが複数回カウントされるのを防ぎます。例えば、まばたきの瞬間にカウントが1増え、その後のカウントが10フレーム以上（約1/3秒以上）経過しない場合、次のまばたきがカウントされます。
    # これらの変数は、まばたきの検出とカウントプロセスを制御し、ユーザーのまばたきの頻度を追跡するために使用されます。まばたきが検出された際に `blinkCounter` が増加し、その後リセットが行われることで、正確なまばたき回数がカウントされます。
    blinkCounter = 0
    counter = 0

    # まばたき状態に応じて表示する色の初期化
    color1 = (255,0,255)
    color2 = (0,200,0)
    color = color1
    
    # 初期のblinklate値
    global blinklate
    blinklate = 34
    
    # ブリンク遅延を調整するトラックバーのコールバック関数
    def on_blinklate_trackbar(val):
        global blinklate
        blinklate = val
        
    # OpenCVウィンドウを作成
    cv2.namedWindow('Blink Detection')
    
    # トラックバーを作成し、初期値を設定
    cv2.createTrackbar('Blink Late', 'Blink Detection', blinklate, 100, on_blinklate_trackbar)

    # 無限ループ
    while True:
        if cap.get(cv2.CAP_PROP_POS_FRAMES) == cap.get(cv2.CAP_PROP_FRAME_COUNT):
            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)

        # ビデオフィードからフレームを取得
        # このコードブロックは、カメラからビデオフィードを読み込み、1つのフレームを取得しています。
        # 1. `cap.read()`: `cap` はOpenCVの `VideoCapture` オブジェクトで、カメラやビデオファイルなどからフレームを読み込むためのインターフェースを提供します。`cap.read()` メソッドを呼び出すことで、カメラから1つのフレームを読み込みます。
        # 2. `success`: フレームの読み込みが成功したかどうかを示すブール値を返します。成功した場合、`success` は `True` に設定され、フレームが正常に読み込まれました。読み込みに失敗した場合、`success` は `False` に設定されます。
        # 3. `img`: 読み込まれたビデオフレームがこの変数に格納されます。このフレームは画像データとして表現され、後続の処理で使用されます。
        # このコードブロックは、カメラから連続的にフレームを読み込み、`success` が `True` の間、ビデオフィードの処理が続けられます。各フレームはループ内で処理され、顔メッシュを検出したり、まばたきのカウントを行ったりするために使用されます。
        # 以上の説明からわかる通り、imgは画像データ！
        success, img = cap.read()

        # 顔メッシュを検出し、ランドマークを描画しないで返す
        # このコードブロックは、ビデオフレームから顔メッシュを検出し、検出した顔メッシュの情報を取得します。また、ランドマークの描画は行わず、検出結果を画像にオーバーレイせずに取得します。
        # 1. `detector.findFaceMesh(img, draw=False)`: `detector` は `FaceMeshDetector` クラスのインスタンスで、顔メッシュを検出するためのツールです。このメソッドは、ビデオフレーム `img` から顔メッシュを検出します。`draw=False` の設定により、顔メッシュの検出結果を画像に描画せずに取得します。描画を行わないことで、単に検出された情報が得られ、画像自体に変更が加えられません。
        #
        # ＜顔メッシュって何＞
        # 顔メッシュ（Face Mesh）は、顔の構造や特徴を3Dモデル化したもの、もしくは、2Dビデオフレームや画像上における顔のランドマーク（特定の顔の部位やポイント）を検出するためのコンピュータビジョン技術の一つです。顔メッシュは主に以下のような目的で使用されます
        #
        # 2. `img`: この変数には、元のビデオフレームに対する顔メッシュの検出情報が含まれた画像が格納されます。
        # 3. `faces`: 顔メッシュの検出結果がこの変数に格納されます。具体的には、`faces` には各顔の情報が含まれたリストが返されます。このコードでは、最大で1つの顔を検出するように設定されているため、`faces` リストには検出された1つの顔の情報が含まれます。
        # このコードブロックは、ビデオフレームから顔メッシュの検出を行い、その結果を `img` と `faces` に格納します。この後の処理で、検出された顔の情報を使用して目のランドマークの描画やまばたきの検出などが行われます。
        img, faces = detector.findFaceMesh(img, draw=False)
        
        if faces:#顔メッシュが検出されたら実行

            # この行のコードは、顔メッシュから検出された顔の情報を取得しています。具体的には、`faces` リストから最初の要素（インデックス0）を取得しています。
            # `faces` リストには、ビデオフレーム内で検出された顔の情報が格納されています。通常、このコードは最初に検出された顔の情報を取得するために使用されます。この情報には、顔のランドマークの位置やその他の顔に関する情報が含まれています。
            # 例えば、ビデオフレーム内に複数の顔がある場合、`faces` リストにはそれぞれの顔に関する情報が含まれており、`faces[0]` は最初の顔に関する情報を表します。この情報を使用して、特定の顔のランドマークを取得したり、顔の特徴を解析したりすることができます。
            # この行のコードは、後続のコードブロックで顔のランドマークや目の角度などの情報を取得するために使用されます。
            face = faces[0]

            # 目のランドマークを描画
            # このコードブロックは、顔メッシュから検出された目のランドマークをビデオフレームに描画するための処理を行っています。具体的な操作は以下の通りです：
            #
            # 1. `for id in idList:`: `idList` は目のランドマークのインデックスのリストです。このループを使用して、各ランドマークに対して以下の処理を行います。
            # 2. `cv2.circle(img, face[id], 5, color, cv2.FILLED)`: このコードは、指定されたランドマークの位置に円を描画します。具体的には、以下のパラメータを使用して円を描画します。
            # - `img`: 描画対象のビデオフレーム。
            # - `face[id]`: 描画する円の中心座標。この座標はランドマークの位置に対応します。
            # - `5`: 円の半径。
            # - `color`: 円の色。この色は変数 `color` で定義されており、まばたきの状態に応じて変わります。
            # - `cv2.FILLED`: 円を塗りつぶすオプション。このオプションを指定することで、円を塗りつぶします。

            # この処理により、指定された目のランドマークに対応する位置に円が描画され、目の位置が可視化されます。目のランドマークはまばたきの検出や顔の姿勢推定などのタスクに使用され、ランドマークの位置をビデオフレーム上で表示することで、その情報を確認できるようになります。また、`color` の色はまばたきの状態に応じて変化し、まばたきの検出状態を可視化するのに役立ちます。
            for id in idList:
                cv2.circle(img, face[id], 5, color, cv2.FILLED)

            # 目の角度を計算
            # このコードブロックでは、顔メッシュから検出された目のランドマークを使用して、目の角度を計算し、それをビデオフレームに線で描画しています。具体的な処理は以下の通りです：
            #
            # 1. `leftUp`、`leftDown`、`leftLeft`、`leftRight` の4つの変数を使用して、目の特定のランドマークの位置を取得しています。これらのランドマークは顔メッシュから検出され、それぞれ目の上部、下部、左側、右側に位置します。
            # 2. `detector.findDistance(leftUp,leftDown)` および `detector.findDistance(leftLeft,leftRight)` を使用して、目の上部と下部、左側と右側の間の距離を計算しています。これらの距離を使用して、目の角度を計算します。
            # 3. `cv2.line(img, leftUp, leftDown, color2, 3)` および `cv2.line(img, leftLeft, leftRight, color2, 3)` を使用して、計算された目の角度に基づいてビデオフレームに直線を描画しています。具体的には、以下のパラメータを指定しています：
            # - `img`: 描画対象のビデオフレーム。
            # - `leftUp` と `leftDown` または `leftLeft` と `leftRight`：直線の始点と終点の座標。
            # - `color2`: 直線の色。この色は変数 `color2` で定義されており、まばたきの状態に応じて変化します。
            # - `3`: 直線の幅（太さ）。
            # これにより、ビデオフレーム上で目の角度が直線で描画され、目の位置や角度が視覚的に表現されます。この情報は、まばたきの検出や顔の姿勢推定などのタスクに役立ちます。また、`color2` の色もまばたきの状態に応じて変わり、まばたきの検出状態を可視化するのに役立ちます。
            # 
            # ＜faceリストの159,23など数は何を意味するのか＞
            # それぞれの番号（159、23、130、243）は、顔メッシュのランドマークのインデックスを表しています。この番号は、FaceMesh モデルが顔の異なる部分を識別し、それぞれの部分に対応するランドマークを持っているため選ばれています。具体的には、以下のような意味があります：
            # - `159`: このランドマークは、左目の上部の位置を示しています。左目の上部の位置を取得するために使用されます。
            # - `23`: このランドマークは、左目の下部の位置を示しています。左目の下部の位置を取得するために使用されます。
            # - `130`: このランドマークは、左目の左側の位置を示しています。左目の左側の位置を取得するために使用されます。
            # - `243`: このランドマークは、左目の右側の位置を示しています。左目の右側の位置を取得するために使用されます。
            # これらのランドマークは、通常、目の位置や角度を計算するために使用されます。例えば、左目の上部と下部の位置を使用して目の縦方向の長さを計算し、左目の左側と右側の位置を使用して目の横方向の長さを計算することができます。これにより、目の角度やまばたきの検出などの情報を取得できます。
            # 顔メッシュの各ランドマークは、顔の異なる部分を表すためにインデックス番号が割り当てられており、それぞれのランドマークは特定の部分の位置情報を提供します。したがって、目的に応じて適切なランドマークを選択し、その位置情報を利用することが重要です。
            leftUp = face[159]
            leftDown = face[23]
            leftLeft = face[130]
            leftRight = face[243]

            
            cv2.line(img, leftUp, leftDown, color2, 3)#???
            cv2.line(img, leftLeft, leftRight, color2, 3)


            lengthVer,_ = detector.findDistance(leftUp,leftDown)#左目の上部 (leftUp) と下部 (leftDown) の間の垂直方向の距離を計算
            lengthHor,_ = detector.findDistance(leftLeft,leftRight)#左目の左側 (leftLeft) と右側 (leftRight) の間の水平方向の距離を計算

            #直線の描写
            # このコードブロックは、計算された目の角度情報を使用して、ビデオフレーム上に直線を描画しています。具体的には、以下の2つの直線が描かれています：
            # 1. `cv2.line(img, leftUp, leftDown, color2, 3)`: この行のコードは、ビデオフレーム `img` 上に、`leftUp` から `leftDown` までの直線を描画しています。これらの座標は、先前計算された左目の上部 (`leftUp`) と下部 (`leftDown`) の位置を指定しています。直線の色は変数 `color2` で指定され、太さは `3` です。
            # 2. `cv2.line(img, leftLeft, leftRight, color2, 3)`: この行のコードは、ビデオフレーム `img` 上に、`leftLeft` から `leftRight` までの直線を描画しています。これらの座標は、左目の左側 (`leftLeft`) と右側 (`leftRight`) の位置を指定しています。直線の色も変数 `color2` で指定され、太さは `3` です。
            # これらの直線は、通常、目の角度やまばたきの状態を視覚的に表現するために使用されます。例えば、目の上下方向の直線はまばたきの状態を示し、目の左右方向の直線は目の角度を示すことがあります。このような視覚的な表現は、アプリケーションやデモンストレーションにおいて、ユーザーに対して情報をわかりやすく伝えるのに役立ちます。また、`color2` の色もまばたきの状態に応じて変化することで、まばたきの検出状態を可視化するのに役立ちます。
            cv2.line(img, leftUp, leftDown, color2, 3)
            cv2.line(img, leftLeft, leftRight, color2, 3)

            # 目の角度からまばたきの比率を計算
            # (lengthVer/lengthHor)*100 がまばたきの比率になる理由は、顔メッシュの特定のランドマーク（目の上部と下部、左側と右側）の位置を使用して、垂直方向と水平方向の長さの比率を計算しているからです。以下にその詳細を説明します：
            # - `lengthVer` は、目の垂直方向の長さを表します。つまり、目の上部から下部までの距離です。
            # - `lengthHor` は、目の水平方向の長さを表します。具体的には、目の左側から右側までの距離です。
            # したがって、(lengthVer/lengthHor) は、目の垂直方向と水平方向の長さの比率を示しています。この比率が 1.0 よりも大きい場合、目の垂直方向の長さが水平方向よりも大きく、まばたきが開いている状態を示します。一方、この比率が 1.0 よりも小さい場合、目の水平方向の長さが垂直方向よりも大きく、まばたきが閉じている状態を示します。
            # 比率をパーセンテージに変換するために、*100 を乗じています。これにより、まばたきの比率が 100% を基準にしてパーセンテージで表されます。たとえば、まばたきが完全に閉じている場合、比率は 0% になり、まばたきが完全に開いている場合、比率は 100% になります。
            # この比率の計算は、目の状態を数値的に表現し、まばたきの状態を検出するための一つの方法です。比率のしきい値を設定し、そのしきい値を下回ったときにまばたきを検出することができます。
            ratio = int((lengthVer/lengthHor)*100)#目が完全に閉じているすなわちlengthVerが0になるときratioは0%になる。（しかし、人体の構造上0%も100%もありえない。あくまで理論的な最小と最大である）
            ratioList.append(ratio)#ratioList.append(ratio) は、まばたきの比率をリスト ratioList に追加する操作

            # 直近の3つの比率を保持し、平均を計算
            if len(ratioList) > 3:
                ratioList.pop(0)
            ratioAvg = sum(ratioList) / len(ratioList)

            # まばたきの検出とカウント
            if ratioAvg < blinklate and counter == 0: # CuddlyScope: 39, Interated Cam: 29
                blinkCounter += 1
                color = color1
                counter = 1
            if counter != 0:
                counter +=1
                if counter > 10:
                    counter = 0
                    color = color2

            # まばたき回数を画面に表示
            # このコード行は、画像上にまばたきの回数を表示するためのものです。以下はこのコードの詳細です：
            #
            # - `cvzone.putTextRect()` 関数は、指定されたテキストを指定された位置に描画するために使用されます。
            # - `img` は画像オブジェクトです。この関数はテキストを描画する対象の画像を指定します。
            # - `'Blink Count: {blinkCounter}'` は表示するテキストです。このテキスト内の `{blinkCounter}` は、`blinkCounter` 変数の値に置き換えられます。したがって、`'Blink Count:'` と `blinkCounter` の現在の値が画像に表示されます。
            # - `(50, 100)` はテキストを表示する位置を指定します。具体的には、テキストの左上隅が `(50, 100)` の位置に配置されます。
            # - `colorR=color` はテキストの色を指定します。この場合、変数 `color` で指定された色が使用されます。
            #
            # このコード行により、画像の指定された位置にまばたきの回数が表示され、表示の色も `color` 変数で指定された色に設定されます。このようにして、まばたきの回数をリアルタイムで画像上に表示できます。
            cvzone.putTextRect(img, f'Blink Count: {blinkCounter}', (50,100), colorR=color)

            # グラフをアップデートし、画像をリサイズ
            # このコードブロックは、まばたきの比率に基づいてグラフをアップデートし、画像をリサイズする部分です。以下に、このコードの詳細を説明します：
            # 1. `plotY.update(ratioAvg, color)` は、まばたきの比率（`ratioAvg`）と色情報（`color`）を使用して、グラフをアップデートするための関数呼び出しです。`plotY` は `LivePlot` オブジェクトで、まばたきの比率をグラフ化するために使用されています。
            # 2. `img = cv2.resize(img, (640,360))` は、キャプチャしたビデオフレームのサイズを `(640, 360)` にリサイズしています。これにより、画像の表示サイズが変更され、ウィンドウ内に収めることができます。
            # このコードブロックにより、まばたきの比率に基づいてリアルタイムでグラフがアップデートされ、画像もリサイズされます。画像は `imgStack` に組み合わせられ、表示されるウィンドウ内で正確に配置されます。
            imgPlot = plotY.update(ratioAvg, color)
            img = cv2.resize(img, (640,360))

            # 画像を積み重ねて表示
            # このコード行は、`img` と `imgPlot` という2つの画像を垂直に積み重ねて新しい画像 `imgStack` を作成するためのものです。以下にこのコードの詳細を説明します：
            # - `cvzone.stackImages()` 関数は、指定した画像を指定した方法で積み重ねるために使用されます。具体的には、指定した画像を水平または垂直に並べることができます。
            # - `img` と `imgPlot` は積み重ねる対象の2つの画像です。`img` はビデオフレームであり、`imgPlot` はまばたきの比率を示すグラフです。
            # - `2` は行の数を指定し、`1` は列の数を指定しています。したがって、`img` と `imgPlot` は垂直に積み重ねられ、2行1列のレイアウトで表示されます。
            # このコードにより、ビデオフレームとまばたきの比率グラフが垂直に積み重ねられて `imgStack` という新しい画像が作成されます。この `imgStack` は、ウィンドウ内で表示され、ビデオフィードとまばたきのグラフが同じ画面上に表示されます。
            imgStack = cvzone.stackImages([img, imgPlot], 2, 1)
        else:
            # 顔が検出されない場合、単にビデオフィードを表示
            # このコードブロックは、ビデオフレーム `img` を指定されたサイズ `(640, 360)` にリサイズし、その後、ビデオフレーム自体を2つ積み重ねて `imgStack` という新しい画像を作成するためのものです。以下にこのコードの詳細を説明します：
            # 1. `cv2.resize(img, (640, 360))` は、ビデオフレーム `img` のサイズを `(640, 360)` にリサイズする部分です。これにより、ビデオフレームのサイズが指定された幅と高さに変更されます。通常、画像をリサイズすることで、ウィンドウ内に収めるための適切な表示サイズを設定するのに役立ちます。
            # 2. `cvzone.stackImages([img, img], 2, 1)` は、指定された画像（ここでは `img` を2回指定しています）を水平または垂直に積み重ねるための関数呼び出しです。`2` は行の数を指定し、`1` は列の数を指定しています。したがって、`img` が2つ並べられ、2行1列のレイアウトで表示されます。ただし、同じビデオフレームが2回積み重ねられているため、視覚的には変化がない状態です。
            # このコードブロックは、ビデオフレームをリサイズし、それを2つ積み重ねているようですが、現在のコードでは視覚的には何も変化がないため、効果がないように見えます。ビデオフレームを表示するためには、`cv2.imshow('Image', imgStack)` の部分で `imgStack` を表示する必要があります。また、`img` を2回積み重ねることは意味がないため、もし意図しているのであれば、別の画像や情報を積み重ねる必要があります。
            img = cv2.resize(img, (640,360))
            imgStack = cvzone.stackImages([img, img], 2, 1)
        
        # 画面にフレームを表示し、キー入力を待機
        # このコードブロックは、OpenCVを使用して画像を表示するためのものです。以下にコードの詳細を説明します：
        # - `cv2.imshow('Image', imgStack)` は、`imgStack` という画像をウィンドウに表示するための関数呼び出しです。`'Image'` はウィンドウのタイトルを指定します。つまり、'Image' というタイトルのウィンドウが作成され、その中に `imgStack` が表示されます。
        # - `cv2.waitKey(1)` は、ウィンドウを表示した後、指定されたミリ秒数（ここでは1ミリ秒）だけキー入力を待機するための関数呼び出しです。この関数は通常、ウィンドウが表示されている間、キー入力を受け付けるために使用されます。1ミリ秒の待機時間を持つことで、ウィンドウが非表示になる前にキー入力を受け付けることができます。
        # このコードブロックにより、`imgStack` という画像が表示されるウィンドウが作成され、そのウィンドウが表示された後に1ミリ秒間キー入力を待機します。ユーザーがキーを押すと、その入力に応じて適切な処理を追加できます。たとえば、ウィンドウを閉じたり、スクリーンショットを保存したりするなどのアクションをトリガーすることができます。
        cv2.imshow('Blink Detection', imgStack)
        cv2.waitKey(1)

        

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/run_script', methods=['POST'])
def run_script():
    # ボタンがクリックされたときに実行するコードをここに書きます
    run_camera()
    return 'Script executed successfully'

if __name__ == "__main__":
    app.run(debug=True)
